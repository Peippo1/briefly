| CI/CD | License | Python | Streamlit | Docker | GCP | Terraform |
|:-----:|:-------:|:------:|:---------:|:------:|:---:|:---------:|
| ![CI](https://github.com/Peippo1/briefly/actions/workflows/ci.yml/badge.svg) | ![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg) | ![Python](https://img.shields.io/badge/python-3.11-blue) | ![Built with Streamlit](https://img.shields.io/badge/Built%20with-Streamlit-ff4b4b?logo=streamlit) | ![Dockerized](https://img.shields.io/badge/Dockerized-Yes-blue?logo=docker) | ![Runs on GCP](https://img.shields.io/badge/Runs%20on-Google%20Cloud-blue?logo=googlecloud) | ![Terraform](https://img.shields.io/badge/Infrastructure-Terraform-623CE4?logo=terraform) |



# ğŸ“° Briefly
ğŸ”— [Live Demo](https://briefly-ai.streamlit.app/)

**Briefly** is a lightweight, AI-powered ETL pipeline that pulls trending news headlines, summarizes them using Google's Gemini API, and displays them in a clean web app interface. It's built with Python, Streamlit, and GCP â€” ideal for showcasing real-time NLP + data engineering skills.

## ğŸš€ Features

- Extract top news stories from Hacker News
- Summarize headlines using Gemini 1.5 Pro
- Display summaries in a dynamic Streamlit app
- Top navigation bar with Feed and Trending views
- Light/Dark theme toggle in the header
- Live date range and source filtering in the sidebar
- Preview logos for each article (with fallback)
- Optional support for BigQuery or CSV export
- Free-tier compatible (Google Gemini 1.5)

## ğŸ§± Tech Stack

- **Python** (ETL scripts)
- **BigQuery** (cloud data warehouse)
- **Gemini API** (summarization)
- **Streamlit** (web UI)
- **Terraform** (infra-as-code, optional)
- **Docker** (optional for app deployment)

## ğŸ“‚ Project Structure

```
briefly/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ summarize.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ webapp/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ databricks_etl.ipynb
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ main.tf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ›  System Requirements

To get started with this project, you'll need the following tools installed:

- [Python 3.11+](https://www.python.org/downloads/)
- [Terraform 1.3+](https://developer.hashicorp.com/terraform/downloads)
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) â€“ required for authenticating with GCP and managing infrastructure
- [Streamlit](https://streamlit.io/) â€“ installed via `pip install -r requirements.txt`

## ğŸ”‘ Environment Setup

1. Clone the repo
2. Create a `.env` file:
   ```
   GEMINI_API_KEY=your-api-key-here
   ```
3. Ensure your Google Cloud credentials are available:
   ```
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service_account.json
   export GCP_PROJECT=your-gcp-project-id
   ```
   # (Required for BigQuery integration)
4. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

## ğŸ§ª Run Locally

```bash
# Create and activate your virtual environment (if needed)
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run full ETL pipeline (extract, summarize, and load into BigQuery)
python etl/run_pipeline.py

# Launch the frontend dashboard
streamlit run webapp/app.py
```

## ğŸ“¡ BigQuery Integration

If you want to store and analyze summaries in BigQuery:

1. Set your GCP credentials and project ID as environment variables:
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service_account.json
   export GCP_PROJECT=your-gcp-project-id
   ```
2. Run the setup script to create the dataset and table:
   ```bash
   python etl/setup_bigquery.py
   ```
3. Use `etl/run_pipeline.py` to automatically push new summaries to BigQuery.

Summaries are stored in the `briefly_data.summaries` table with fields like `url`, `title`, `summary`, `source`, `published_at`, and `summarized_at`.

## ğŸ—ï¸ Terraform Infrastructure

You can provision the required GCP infrastructure using Terraform:

1. Navigate to the Terraform directory:
   ```bash
   cd terraform/
   ```

2. Set your environment credentials (if not already):
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS=./.secrets/terraform-admin-key.json
   ```

3. Initialize the Terraform project:
   ```bash
   terraform init
   ```

4. Review the plan:
   ```bash
   terraform plan
   ```

5. Apply the infrastructure:
   ```bash
   terraform apply
   ```

Terraform will create:
- A BigQuery dataset and summaries table
- A service account with `bigquery.user` permissions
 - GitHub Actions CI/CD validation pipeline

## ğŸ§¹ Terraform Cleanup and Remote Backend (Optional)

### Destroy Infrastructure

To tear down all Terraform-managed resources:

```bash
terraform destroy
```

This will prompt you to confirm deletion of all provisioned infrastructure.

---

### Use a Remote Backend (Optional but Recommended)

For team collaboration and state consistency, configure a remote backend using Google Cloud Storage (GCS):

1. Create a GCS bucket (e.g. `briefly-terraform-state`)
2. Enable versioning on the bucket:
   ```bash
   gsutil versioning set on gs://briefly-terraform-state
   ```

3. Add a backend config to your `provider.tf` or `main.tf`:

```hcl
terraform {
  backend "gcs" {
    bucket  = "briefly-terraform-state"
    prefix  = "terraform/state"
  }
}
```

4. Reinitialize Terraform to migrate local state:
```bash
terraform init -migrate-state
```

This ensures your Terraform state is versioned, backed up, and team-ready.

## ğŸ“œ License

MIT â€” free to use, extend, and showcase.

## âœ… Project Status

This project is complete and production-ready. Further improvements (e.g. CI deployment, testing automation, or remote backends) can be added as future enhancements.